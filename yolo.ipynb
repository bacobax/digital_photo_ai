{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "456f75d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/francescobassignana/digital_photo_ai/test_image.jpg: 640x480 1 FACE, 304.3ms\n",
      "Speed: 13.7ms preprocess, 304.3ms inference, 126.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import argparse\n",
    "\n",
    "# Parse the command line arguments\n",
    "#variables = parse_variables()\n",
    "\n",
    "# Select device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'mps' if torch.backends.mps.is_available() else device\n",
    "\n",
    "img_path = './test_image.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "H, W = img.shape[:2]\n",
    "\n",
    "# Load the pretrained model\n",
    "model = YOLO(\"./yolo-face.pt\")                \n",
    "model.to(device)\n",
    "\n",
    "# Run inference on the webcam\n",
    "res = model.predict('./test_image.jpg', verbose=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37ac2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "657a0f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0.], device='mps:0')\n",
       "conf: tensor([0.8970], device='mps:0')\n",
       "data: tensor([[1.1184e+03, 1.4589e+03, 1.9080e+03, 2.5367e+03, 8.9697e-01, 0.0000e+00]], device='mps:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (4032, 3024)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[1513.1968, 1997.7686,  789.5991, 1077.8091]], device='mps:0')\n",
       "xywhn: tensor([[0.5004, 0.4955, 0.2611, 0.2673]], device='mps:0')\n",
       "xyxy: tensor([[1118.3972, 1458.8640, 1907.9963, 2536.6731]], device='mps:0')\n",
       "xyxyn: tensor([[0.3698, 0.3618, 0.6310, 0.6291]], device='mps:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f31aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xywh = res.boxes.xywh\n",
    "w = xywh[0,2]\n",
    "h = xywh[0,3]\n",
    "ratio = w / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a682e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adjust_box_to_ratio(x1, y1, x2, y2, img_w, img_h,\n",
    "                        target_w_over_h=7/9, strategy=\"auto\"):\n",
    "    \"\"\"\n",
    "    Adjust an xyxy box to a target aspect ratio (w:h), preserving center.\n",
    "\n",
    "    Args:\n",
    "        x1,y1,x2,y2: box in pixels (floats ok)\n",
    "        img_w, img_h: image size\n",
    "        target_w_over_h: desired width/height ratio (e.g., 7/9 for portrait)\n",
    "        strategy: \"expand\", \"shrink\", or \"auto\"\n",
    "            - expand: only increase width or height\n",
    "            - shrink: only decrease width or height\n",
    "            - auto: change the dimension (w or h) that needs the smallest delta (may expand or shrink)\n",
    "\n",
    "    Returns:\n",
    "        nx1, ny1, nx2, ny2 (floats)\n",
    "    \"\"\"\n",
    "    if target_w_over_h <= 0:\n",
    "        raise ValueError(\"target_w_over_h must be > 0\")\n",
    "\n",
    "    # center + current size\n",
    "    cx = (x1 + x2) / 2.0\n",
    "    cy = (y1 + y2) / 2.0\n",
    "    w  = max(1.0, x2 - x1)\n",
    "    h  = max(1.0, y2 - y1)\n",
    "\n",
    "    cur_ratio = w / h\n",
    "    if abs(cur_ratio - target_w_over_h) < 1e-9:\n",
    "        # Already at ratio; just shift/clamp to be safe\n",
    "        nx1, ny1, nx2, ny2 = cx - w/2, cy - h/2, cx + w/2, cy + h/2\n",
    "    else:\n",
    "        # Candidate 1: change width only (w' = target * h)\n",
    "        w_from_h = target_w_over_h * h\n",
    "        # Candidate 2: change height only (h' = w / target)\n",
    "        h_from_w = w / target_w_over_h\n",
    "\n",
    "        if strategy == \"expand\":\n",
    "            if cur_ratio < target_w_over_h:\n",
    "                # too tall -> widen\n",
    "                w = max(w, w_from_h)\n",
    "            else:\n",
    "                # too wide -> make taller\n",
    "                h = max(h, h_from_w)\n",
    "        elif strategy == \"shrink\":\n",
    "            if cur_ratio < target_w_over_h:\n",
    "                # too tall -> reduce height\n",
    "                h = min(h, h_from_w)\n",
    "            else:\n",
    "                # too wide -> reduce width\n",
    "                w = min(w, w_from_h)\n",
    "        else:  # \"auto\": minimal change to one side\n",
    "            dw = abs(w_from_h - w)\n",
    "            dh = abs(h_from_w - h)\n",
    "            if dw <= dh:\n",
    "                w = w_from_h\n",
    "            else:\n",
    "                h = h_from_w\n",
    "\n",
    "        nx1, ny1 = cx - w / 2.0, cy - h / 2.0\n",
    "        nx2, ny2 = cx + w / 2.0, cy + h / 2.0\n",
    "\n",
    "    # Shift inside image (preserve size)\n",
    "    dx_left  = max(0.0, -nx1)\n",
    "    dx_right = max(0.0, nx2 - img_w)\n",
    "    dy_top   = max(0.0, -ny1)\n",
    "    dy_bot   = max(0.0, ny2 - img_h)\n",
    "\n",
    "    nx1 += (dx_left - dx_right); nx2 += (dx_left - dx_right)\n",
    "    ny1 += (dy_top - dy_bot);    ny2 += (dy_top - dy_bot)\n",
    "\n",
    "    # If still out (box bigger than image), scale down uniformly to fit\n",
    "    new_w = nx2 - nx1\n",
    "    new_h = ny2 - ny1\n",
    "    if new_w > img_w or new_h > img_h:\n",
    "        s = min(img_w / new_w, img_h / new_h, 1.0)\n",
    "        new_w *= s; new_h *= s\n",
    "        nx1 = cx - new_w / 2.0; nx2 = cx + new_w / 2.0\n",
    "        ny1 = cy - new_h / 2.0; ny2 = cy + new_h / 2.0\n",
    "        # shift again just in case of fp rounding\n",
    "        dx_left  = max(0.0, -nx1)\n",
    "        dx_right = max(0.0, nx2 - img_w)\n",
    "        dy_top   = max(0.0, -ny1)\n",
    "        dy_bot   = max(0.0, ny2 - img_h)\n",
    "        nx1 += (dx_left - dx_right); nx2 += (dx_left - dx_right)\n",
    "        ny1 += (dy_top - dy_bot);    ny2 += (dy_top - dy_bot)\n",
    "\n",
    "    # Final safety clamp\n",
    "    nx1 = float(np.clip(nx1, 0, img_w - 1))\n",
    "    ny1 = float(np.clip(ny1, 0, img_h - 1))\n",
    "    nx2 = float(np.clip(nx2, 0, img_w - 1))\n",
    "    ny2 = float(np.clip(ny2, 0, img_h - 1))\n",
    "    if nx2 <= nx1: nx2 = min(img_w - 1.0, nx1 + 1.0)\n",
    "    if ny2 <= ny1: ny2 = min(img_h - 1.0, ny1 + 1.0)\n",
    "\n",
    "    return nx1, ny1, nx2, ny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c061e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_xyxy = res.boxes.xyxy.detach().cpu().numpy() if res.boxes is not None else np.empty((0,4))\n",
    "adjusted = []\n",
    "for (x1, y1, x2, y2) in boxes_xyxy:\n",
    "    ax1, ay1, ax2, ay2 = adjust_box_to_ratio(x1, y1, x2, y2, img_w=W, img_h=H,\n",
    "                                         target_w_over_h=7/9, strategy=\"auto\")\n",
    "\n",
    "    adjusted.append((int(round(ax1)), int(round(ay1)), int(round(ax2)), int(round(ay2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio : 0.777\n",
      "old ratio : 0.733\n"
     ]
    }
   ],
   "source": [
    "# --- draw adjusted boxes ---\n",
    "vis = img.copy()\n",
    "for (x1, y1, x2, y2), box in zip(adjusted, boxes_xyxy):\n",
    "    cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 3)  # green box\n",
    "    print(f\"ratio : {(x2 - x1) / (y2 - y1):.3f}\")\n",
    "    # optional: draw original box in a thin line to compare\n",
    "    cv2.rectangle(vis, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255, 0, 0), 1)\n",
    "    print(f\"old ratio : {(box[2] - box[0]) / (box[3] - box[1]):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acff38e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: faces_7x9.jpg  | boxes: [(1094, 1459, 1932, 2537)]\n"
     ]
    }
   ],
   "source": [
    "cv2.imwrite('faces_7x9.jpg', vis)\n",
    "print(f\"Saved: faces_7x9.jpg  | boxes: {adjusted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d26036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixels per mm: 2.83 x 2.83\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31ed05d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved faces_7x9_min35x45mm.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "img_path = \"./test_image.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "H, W = img.shape[:2]\n",
    "\n",
    "# You already have x_ppi, y_ppi (pixels per inch). Example:\n",
    "# x_ppi, y_ppi = 300, 300\n",
    "# If you computed pixels/mm instead, multiply mm by ppmm directly (skip /25.4).\n",
    "\n",
    "model = YOLO(\"./yolo-face.pt\")\n",
    "res = model.predict(img_path, verbose=False)[0]\n",
    "\n",
    "boxes_xyxy = res.boxes.xyxy.detach().cpu().numpy() if res.boxes is not None else np.empty((0,4))\n",
    "\n",
    "vis = img.copy()\n",
    "for (x1, y1, x2, y2) in boxes_xyxy:\n",
    "    ax1, ay1, ax2, ay2 = adjust_box_to_ratio(x1, y1, x2, y2, img_w=W, img_h=H,\n",
    "                                         target_w_over_h=7/9, strategy=\"auto\")\n",
    "\n",
    "    # draw adjusted box\n",
    "    cv2.rectangle(vis, (int(round(ax1)), int(round(ay1))),\n",
    "                        (int(round(ax2)), int(round(ay2))),\n",
    "                        (0,255,0), 3)\n",
    "    if warn:\n",
    "        print(\"⚠️ Required 35×45 mm box cannot fully fit inside the image; clamped to image bounds.\")\n",
    "\n",
    "cv2.imwrite(\"faces_7x9_min35x45mm.jpg\", vis)\n",
    "print(\"Saved faces_7x9_min35x45mm.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f25e5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: helpers + imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def colorize_grabcut_mask(mask):\n",
    "    \"\"\"\n",
    "    Color map for GrabCut labels:\n",
    "      0: GC_BGD      -> red-ish\n",
    "      1: GC_FGD      -> green\n",
    "      2: GC_PR_BGD   -> orange\n",
    "      3: GC_PR_FGD   -> cyan\n",
    "    \"\"\"\n",
    "    h, w = mask.shape[:2]\n",
    "    out = np.zeros((h, w, 3), np.uint8)\n",
    "    out[mask == cv2.GC_BGD]    = (0,   0, 200)   # BGR\n",
    "    out[mask == cv2.GC_FGD]    = (0, 200,   0)\n",
    "    out[mask == cv2.GC_PR_BGD] = (0, 165, 255)\n",
    "    out[mask == cv2.GC_PR_FGD] = (255, 255, 0)\n",
    "    return out\n",
    "\n",
    "def overlay_mask(base, mask_bin, alpha=0.5):\n",
    "    \"\"\"Overlay a white binary mask on base image.\"\"\"\n",
    "    base = base.copy()\n",
    "    white = np.full_like(base, 255)\n",
    "    sel = mask_bin > 0\n",
    "    base[sel] = cv2.addWeighted(base[sel], 1 - alpha, white[sel], alpha, 0)\n",
    "    return base\n",
    "\n",
    "def put_caption(img, text):\n",
    "    \"\"\"Add a black bar caption at the top of an image.\"\"\"\n",
    "    img = img.copy()\n",
    "    cv2.rectangle(img, (0, 0), (img.shape[1], 28), (0, 0, 0), -1)\n",
    "    cv2.putText(img, text, (8, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "# Where to save the debug images:\n",
    "out_dir = \"hair_debug\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8c3b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_of_hair_y_debug(img, box_xyxy, img_w, img_h,\n",
    "                        top_pad_ratio=1.4, side_pad_ratio=0.35,\n",
    "                        below_pad_ratio=0.2,\n",
    "                        band_rel_width=0.35,\n",
    "                        border_bg_px=15,\n",
    "                        grabcut_iters=6):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_top_global (int or None),\n",
    "      debug dict with:\n",
    "        'roi_bgr', 'seed_mask_color', 'gc_mask_color',\n",
    "        'fg_only_overlay', 'main_comp_overlay', 'band_overlay'\n",
    "    \"\"\"\n",
    "    debug = {}\n",
    "    x1, y1, x2, y2 = [int(round(v)) for v in box_xyxy]\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    if w <= 0 or h <= 0:\n",
    "        return None, debug\n",
    "\n",
    "    # ROI bounds\n",
    "    roi_x1 = max(0, x1 - int(side_pad_ratio * w))\n",
    "    roi_x2 = min(img_w, x2 + int(side_pad_ratio * w))\n",
    "    roi_y1 = max(0, y1 - int(top_pad_ratio * h))\n",
    "    roi_y2 = min(img_h, y2 + int(below_pad_ratio * h))\n",
    "\n",
    "    roi = img[roi_y1:roi_y2, roi_x1:roi_x2]\n",
    "    if roi.size == 0:\n",
    "        return None, debug\n",
    "    rh, rw = roi.shape[:2]\n",
    "    debug['roi_bgr'] = put_caption(roi, \"ROI (extended above face)\")\n",
    "\n",
    "    # Seed mask\n",
    "    mask = np.full((rh, rw), cv2.GC_PR_BGD, np.uint8)\n",
    "    # definite BG border\n",
    "    mask[:border_bg_px, :] = cv2.GC_BGD\n",
    "    mask[-border_bg_px:, :] = cv2.GC_BGD\n",
    "    mask[:, :border_bg_px] = cv2.GC_BGD\n",
    "    mask[:, -border_bg_px:] = cv2.GC_BGD\n",
    "\n",
    "    # Face inside ROI\n",
    "    fx1, fy1 = x1 - roi_x1, y1 - roi_y1\n",
    "    fx2, fy2 = x2 - roi_x1, y2 - roi_y1\n",
    "    fw, fh = fx2 - fx1, fy2 - fy1\n",
    "    if fw <= 0 or fh <= 0:\n",
    "        return None, debug\n",
    "\n",
    "    # Definite FG ellipse for face\n",
    "    face_center = (int((fx1 + fx2) / 2), int((fy1 + fy2) / 2))\n",
    "    axes = (max(1, int(0.45 * fw)), max(1, int(0.55 * fh)))\n",
    "    ellipse_mask = np.zeros_like(mask, np.uint8)\n",
    "    cv2.ellipse(ellipse_mask, face_center, axes, 0, 0, 360, 1, -1)\n",
    "    mask[ellipse_mask == 1] = cv2.GC_FGD\n",
    "\n",
    "    # Probable FG strip above the face to encourage hair\n",
    "    hair_top_y = max(0, int(fy1 - 0.1 * fh))\n",
    "    hx1 = max(0, int(fx1 + 0.08 * fw))\n",
    "    hx2 = min(rw, int(fx2 - 0.08 * fw))\n",
    "    hy1 = max(0, hair_top_y - int(0.5 * fh))         # push higher\n",
    "    hy2 = max(0, int(fy1 - 0.02 * fh))\n",
    "    if hx2 > hx1 and hy2 > hy1:\n",
    "        # Don't overwrite definite BG; where mask is BG keep BG, else set PR_FGD\n",
    "        region = mask[hy1:hy2, hx1:hx2]\n",
    "        region = np.where(region == cv2.GC_BGD, cv2.GC_BGD, cv2.GC_PR_FGD)\n",
    "        mask[hy1:hy2, hx1:hx2] = region\n",
    "\n",
    "    debug['seed_mask_color'] = put_caption(colorize_grabcut_mask(mask), \"Seed mask (BG/FG/PR_BG/PR_FG)\")\n",
    "\n",
    "    # Run GrabCut with seeds\n",
    "    bgdModel = np.zeros((1, 65), np.float64)\n",
    "    fgdModel = np.zeros((1, 65), np.float64)\n",
    "    try:\n",
    "        cv2.grabCut(roi, mask, None, bgdModel, fgdModel, grabcut_iters, cv2.GC_INIT_WITH_MASK)\n",
    "    except cv2.error:\n",
    "        return None, debug\n",
    "\n",
    "    debug['gc_mask_color'] = put_caption(colorize_grabcut_mask(mask), \"GrabCut output labels\")\n",
    "\n",
    "    # Foreground (definite or probable)\n",
    "    fg = np.where((mask == cv2.GC_FGD) | (mask == cv2.GC_PR_FGD), 1, 0).astype(np.uint8)\n",
    "    debug['fg_only_overlay'] = put_caption(overlay_mask(roi, fg), \"FG overlay (all FG)\")\n",
    "\n",
    "    # Keep only component intersecting center line (within a central band)\n",
    "    cx_global = int((x1 + x2) / 2)\n",
    "    cx_roi = int(np.clip(cx_global - roi_x1, 0, rw - 1))\n",
    "    band_half = max(1, int(0.5 * band_rel_width * rw))\n",
    "    bx1 = max(0, cx_roi - band_half)\n",
    "    bx2 = min(rw, cx_roi + band_half)\n",
    "\n",
    "    num_labels, labels = cv2.connectedComponents(fg, connectivity=4)\n",
    "    if num_labels <= 1:\n",
    "        return None, debug\n",
    "\n",
    "    band_labels = labels[:, bx1:bx2]\n",
    "    counts = np.bincount(band_labels.reshape(-1), minlength=num_labels)\n",
    "    counts[0] = 0\n",
    "    main_label = np.argmax(counts)\n",
    "    if counts[main_label] == 0:\n",
    "        return None, debug\n",
    "\n",
    "    main_comp = (labels == main_label).astype(np.uint8)\n",
    "\n",
    "    main_overlay = roi.copy()\n",
    "    sel = main_comp.astype(bool)\n",
    "\n",
    "    # Blend selected pixels toward white without cv2.addWeighted shape issues\n",
    "    # new = 0.4 * original + 0.6 * 255\n",
    "    main_overlay[sel] = (0.4 * main_overlay[sel] + 0.6 * 255).astype(np.uint8)\n",
    "\n",
    "    # show central band\n",
    "    cv2.rectangle(main_overlay, (bx1, 0), (bx2, rh - 1), (255, 0, 255), 1)\n",
    "    debug['main_comp_overlay'] = put_caption(main_overlay, \"Main FG component (center-band)\")\n",
    "    # Topmost FG row within band\n",
    "    band = main_comp[:, bx1:bx2]\n",
    "    rows = np.where(band.any(axis=1))[0]\n",
    "    if rows.size == 0:\n",
    "        return None, debug\n",
    "\n",
    "    y_top_in_roi = int(rows[0])\n",
    "    y_top_global = roi_y1 + y_top_in_roi\n",
    "\n",
    "    band_overlay = main_overlay.copy()\n",
    "    cv2.line(band_overlay, (0, y_top_in_roi), (rw - 1, y_top_in_roi), (0, 0, 255), 2)\n",
    "    debug['band_overlay'] = put_caption(band_overlay, \"Central band + detected top line\")\n",
    "\n",
    "    return y_top_global, debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-step visuals to: hair_debug\n"
     ]
    }
   ],
   "source": [
    "vis = img.copy()\n",
    "for i, (x1, y1, x2, y2) in enumerate(boxes_xyxy, start=1):\n",
    "    y_top, dbg = top_of_hair_y_debug(img, (x1, y1, x2, y2), W, H)\n",
    "\n",
    "    # draw original face box\n",
    "    cv2.rectangle(vis, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 3)\n",
    "\n",
    "    if y_top is not None:\n",
    "        cx = int(round((x1 + x2) / 2.0))\n",
    "        cv2.circle(vis, (cx, int(y_top)), 6, (0, 0, 255), -1)\n",
    "        cv2.line(vis, (0, int(y_top)), (W - 1, int(y_top)), (0, 0, 255), 2)\n",
    "    else:\n",
    "        print(f\"[Face {i}] Could not estimate top-of-hair.\")\n",
    "\n",
    "    # Save per-step visuals for this face (if present)\n",
    "    steps = ['roi_bgr', 'seed_mask_color', 'gc_mask_color', 'fg_only_overlay',\n",
    "             'main_comp_overlay', 'band_overlay']\n",
    "    for s in steps:\n",
    "        if s in dbg\n",
    "            cv2.imwrite(os.path.join(out_dir, f\"face{i:02d}_{s}.png\"), dbg[s])\n",
    "\n",
    "cv2.imwrite(os.path.join(out_dir, \"result_with_top_lines.jpg\"), vis)\n",
    "print(f\"Saved per-step visuals to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09813ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:665: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m vis \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x1, y1, x2, y2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(boxes_xyxy, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 178\u001b[0m     y_top, dbg \u001b[38;5;241m=\u001b[39m \u001b[43mtop_of_hair_y_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# draw original face box\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(vis, (\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1)), (\u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2)), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 141\u001b[0m, in \u001b[0;36mtop_of_hair_y_debug\u001b[0;34m(img, box_xyxy, img_w, img_h, top_pad_ratio, side_pad_ratio, below_pad_ratio, band_rel_width, border_bg_px, grabcut_iters)\u001b[0m\n\u001b[1;32m    139\u001b[0m main_comp \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m==\u001b[39m main_label)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    140\u001b[0m main_overlay \u001b[38;5;241m=\u001b[39m roi\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 141\u001b[0m main_overlay[main_comp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddWeighted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_overlay\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmain_comp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(main_overlay, (bx1, \u001b[38;5;241m0\u001b[39m), (bx2, rh \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# show band\u001b[39;00m\n\u001b[1;32m    144\u001b[0m debug[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_comp_overlay\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m put_caption(main_overlay, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain FG component (center-band)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.12.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/core/src/arithm.cpp:665: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff3520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_portrait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
